The selection of bacterial resistance was examined in relationship to antibiotic pharmacokinetics (PK) and organism MICs in the patients from four nosocomial lower respiratory tract infection clinical trials. The evaluable database included 107 acutely ill patients, 128 pathogens, and five antimicrobial regimens. Antimicrobial pharmacokinetics were characterized by using serum concentrations, and culture and sensitivity tests were performed daily on tracheal aspirates to examine resistance. Pharmacodynamic (PD) models were developed to identify factors associated with the probability of developing bacterial resistance. Overall, in 32 of 128 (25%) initially susceptible cases resistance developed during therapy. An initial univariate screen and a classification and regression tree analysis identified the ratio of the area under the concentration-time curve from 0 to 24 h to the MIC (AUC[0-24]/MIC) as a significant predictor of the development of resistance (P < 0.001). The final PK/PD model, a variant of the Hill equation, demonstrated that the probability of developing resistance during therapy increased significantly when antimicrobial exposure was at an AUC[0-24]/MIC ratio of less than 100. This relationship was observed across all treatments and within all organism groupings, with the exception of beta-lactamase-producing gram-negative organisms (consistent with type I beta-lactamase producers) treated with beta-lactam monotherapy. Combination therapy resulted in much lower rates of resistance than monotherapy, probably because all of the combination regimens examined had an AUC[0-24]/MIC ratio in excess of 100. In summary, the selection of antimicrobial resistance appears to be strongly associated with suboptimal antimicrobial exposure, defined as an AUC[0-24]MIC ratio of less than 100.